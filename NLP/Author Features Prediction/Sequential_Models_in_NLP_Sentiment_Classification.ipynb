{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Questions - Project 1 - Sequential Models in NLP - Sentiment Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXaFSkUu0fzm"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1UXScsVx_Wni_JuDdB8LeTnM6jsPfIwkW)\n",
        "\n",
        "Proprietary content. Â© Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OudB5by50jlI"
      },
      "source": [
        "# Sentiment Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT7MKZuMRaCg"
      },
      "source": [
        "### Dataset\n",
        "- Dataset of 50,000 movie reviews from IMDB, labeled by sentiment positive (1) or negative (0)\n",
        "- Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers).\n",
        "- For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
        "- As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
        "\n",
        "Command to import data\n",
        "- `from tensorflow.keras.datasets import imdb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q34-Y3nRKXdO"
      },
      "source": [
        "### Import the data (4 Marks)\n",
        "- Use `imdb.load_data()` method\n",
        "- Get train and test set\n",
        "- Take 10000 most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlBSbyZ-uncp"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow\n",
        "tensorflow.__version__\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import models, layers\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Input\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import  Flatten\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxfwbrbuKbk2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d9012c3-cf35-4d7e-94dc-2f31d4042c52"
      },
      "source": [
        "#### Add your code here ####\n",
        "from tensorflow.keras.datasets import imdb\n",
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omMxqWS8vg36",
        "outputId": "28113c36-a3e2-4804-b7a0-4ce8d14b27dc"
      },
      "source": [
        "print(training_data.shape) \n",
        "print(testing_data.shape)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DldivBO4LTbP"
      },
      "source": [
        "### Pad each sentence to be of same length (4 Marks)\n",
        "- Take maximum sequence length as 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yiya_IEwZrg",
        "outputId": "9209e89b-691c-4d19-e7ec-c6e3a028c8c6"
      },
      "source": [
        "print(\"Categories:\", np.unique(training_targets)) ## remove \n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(training_data))))\n",
        "length = [len(i) for i in training_data]\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Categories: [0 1]\n",
            "Number of unique words: 9998\n",
            "Average Review length: 238.71364\n",
            "Standard Deviation: 176.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E808XB4tLtic"
      },
      "source": [
        "## padding each sentence to be of same length\n",
        "max_sequence_length=300\n",
        "padded_inputs = pad_sequences(training_data, maxlen=max_sequence_length,padding=\"post\") \n",
        "padded_inputs_test = pad_sequences(testing_data, maxlen=max_sequence_length,padding=\"post\") \n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBFFCrybMSXz"
      },
      "source": [
        "### Print shape of features & labels (4 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOcyRtZfMYZd"
      },
      "source": [
        "Number of review, number of words in each review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdMCUPr7RaCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9acdd881-e669-402d-c6ed-eeecfbb30235"
      },
      "source": [
        "\n",
        "print('For training...')\n",
        "print(padded_inputs.shape)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For training...\n",
            "(25000, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGVHeKOWyJiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14dbcc63-309b-4598-a366-e3e672a4405e"
      },
      "source": [
        "\n",
        "print('For testing...')\n",
        "print(padded_inputs_test.shape)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For testing...\n",
            "(25000, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cNk5sDvMr3j"
      },
      "source": [
        "Number of labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z00-mYgMoKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc059028-8320-4f29-b5d0-44a69d63113a"
      },
      "source": [
        "\n",
        "print('For training lables ...')\n",
        "print(training_targets.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For training lables ...\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7f5tPeaMxti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05562701-33af-4ce3-c1f0-9885a45617aa"
      },
      "source": [
        "\n",
        "print('For testing lables...')\n",
        "print(testing_targets.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For testing lables...\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdXPWuOmNEbh"
      },
      "source": [
        "### Print value of any one feature and it's label (4 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGLEdeFmNZfR"
      },
      "source": [
        "Feature value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKFyMa28zztL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c9a1ad0-e785-4e7b-cee5-f8f775cc1554"
      },
      "source": [
        "\n",
        "print(padded_inputs[100])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   1   13  244    6   87  337    7  628 2219    5   28  285   15  240\n",
            "   93   23  288  549   18 1455  673    4  241  534 3635 8448   20   38\n",
            "   54   13  258   46   44   14   13 1241 7258   12    5    5   51    9\n",
            "   14   45    6  762    7    2 1309  328    5  428 2473   15   26 1292\n",
            "    5 3939 6728    5 1960  279   13   92  124  803   52   21  279   14\n",
            "    9   43    6  762    7  595   15   16    2   23    4 1071  467    4\n",
            "  403    7  628 2219    8   97    6  171 3596   99  387   72   97   12\n",
            "  788   15   13  161  459   44    4 3939 1101  173   21   69    8  401\n",
            "    2    4  481   88   61 4731  238   28   32   11   32   14    9    6\n",
            "  545 1332  766    5  203   73   28   43   77  317   11    4    2  953\n",
            "  270   17    6 3616   13  545  386   25   92 1142  129  278   23   14\n",
            "  241   46    7  158    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_85Hqm0Nb1I"
      },
      "source": [
        "Label value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FoehB5jNd1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f0dc2f-629f-42b7-bf4f-83524b51e092"
      },
      "source": [
        "\n",
        "print(testing_targets[100]) "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cof4LSxNxuv"
      },
      "source": [
        "### Decode the feature value to get original sentence (4 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_oiAyPZOkJD"
      },
      "source": [
        "First, retrieve a dictionary that contains mapping of words to their index in the IMDB dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clsk-yK8OtzD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d6347d9-f13f-4330-dba9-7ac050a11b8d"
      },
      "source": [
        "\n",
        "index = imdb.get_word_index()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRgOD5S2Uuvd"
      },
      "source": [
        "Now use the dictionary to get the original words from the encodings, for a particular sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ504QDORwxj"
      },
      "source": [
        "\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
        "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in training_data[100]] )\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLGABrJoVZe6"
      },
      "source": [
        "Get the sentiment for the above sentence\n",
        "- positive (1)\n",
        "- negative (0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68f0PPMv-Tqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8955aaae-3293-4fa5-f545-10ca94d3039a"
      },
      "source": [
        "print(decoded) \n",
        "print(training_targets[100])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# i am a great fan of david lynch and have everything that he's made on dvd except for hotel room the 2 hour twin peaks movie so when i found out about this i immediately grabbed it and and what is this it's a bunch of # drawn black and white cartoons that are loud and foul mouthed and unfunny maybe i don't know what's good but maybe this is just a bunch of crap that was # on the public under the name of david lynch to make a few bucks too let me make it clear that i didn't care about the foul language part but had to keep # the sound because my neighbors might have all in all this is a highly disappointing release and may well have just been left in the # box set as a curiosity i highly recommend you don't spend your money on this 2 out of 10\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmCjr8miXIWB"
      },
      "source": [
        "### Define model (10 Marks)\n",
        "- Define a Sequential Model\n",
        "- Add Embedding layer\n",
        "  - Embedding layer turns positive integers into dense vectors of fixed size\n",
        "  - `tensorflow.keras` embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unique integer number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn LabelEncoder.\n",
        "  - Size of the vocabulary will be 10000\n",
        "  - Give dimension of the dense embedding as 100\n",
        "  - Length of input sequences should be 300\n",
        "- Add LSTM layer\n",
        "  - Pass value in `return_sequences` as True\n",
        "- Add a `TimeDistributed` layer with 100 Dense neurons\n",
        "- Add Flatten layer\n",
        "- Add Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk9NDg_YR6Nf",
        "outputId": "a6d80d7c-43c6-438c-f1a7-a117c7757ee4"
      },
      "source": [
        "\n",
        "\n",
        "max_features=10000\n",
        "\n",
        "mod3 = Sequential()\n",
        "mod3.add(Embedding(max_features, 100))\n",
        "mod3.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1)))\n",
        "mod3.add(TimeDistributed(Dense(100)))\n",
        "mod3.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc4bknOobDby"
      },
      "source": [
        "### Compile the model (4 Marks)\n",
        "- Use Optimizer as Adam\n",
        "- Use Binary Crossentropy as loss\n",
        "- Use Accuracy as metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw4RJ0CQbwFY"
      },
      "source": [
        "\n",
        "mod3.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPyhjyBQMB2P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sEzwazqbz3T"
      },
      "source": [
        "### Print model summary (4 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx1yxwlb2Ue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd73d45f-00a3-4e4a-f1fe-486381d41056"
      },
      "source": [
        "\n",
        "mod3.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (None, None, 100)         1000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_14 (Bidirectio (None, None, 200)         160800    \n",
            "_________________________________________________________________\n",
            "time_distributed_14 (TimeDis (None, None, 100)         20100     \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, None, 1)           101       \n",
            "=================================================================\n",
            "Total params: 1,181,001\n",
            "Trainable params: 1,181,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmkolKP4b-U6"
      },
      "source": [
        "### Fit the model (4 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRg3KFXLcAkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f6362fe-51c1-4038-993e-c0f56da8a173"
      },
      "source": [
        "\n",
        "mod3.fit(padded_inputs, training_targets,batch_size=32,epochs=3,validation_split=0.1, verbose=1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "704/704 [==============================] - 1304s 2s/step - loss: 0.6135 - accuracy: 0.6519 - val_loss: 0.6378 - val_accuracy: 0.6610\n",
            "Epoch 2/3\n",
            "704/704 [==============================] - 1317s 2s/step - loss: 0.5663 - accuracy: 0.6873 - val_loss: 0.5468 - val_accuracy: 0.7076\n",
            "Epoch 3/3\n",
            "704/704 [==============================] - 1322s 2s/step - loss: 0.4640 - accuracy: 0.7562 - val_loss: 0.4788 - val_accuracy: 0.7426\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0d4772ef28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwLl54MXnkEA"
      },
      "source": [
        "### Evaluate model (4 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUqY-bD8RaDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbd250f-bcd6-409d-d0f9-9d2807e4edf0"
      },
      "source": [
        "\n",
        "score, acc = mod3.evaluate(padded_inputs_test, testing_targets,\n",
        "                            batch_size=32)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 118s 151ms/step - loss: 0.4988 - accuracy: 0.7231\n",
            "Test score: 0.4988357424736023\n",
            "Test accuracy: 0.7230520248413086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnXe8fJn0-nQ",
        "outputId": "5a0b150a-853e-4707-b1e0-a26e7d6205bf"
      },
      "source": [
        "score1, acc1 = mod3.evaluate(padded_inputs_test, testing_targets,\n",
        "                            batch_size=32)\n",
        "print('Test score:', score1)\n",
        "print('Test accuracy:', acc1)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 112s 143ms/step - loss: 0.4988 - accuracy: 0.7231\n",
            "Test score: 0.4988357424736023\n",
            "Test accuracy: 0.7230520248413086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2amr1tJn9Jz"
      },
      "source": [
        "### Predict on one sample (4 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl4idfWR_A8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bbc16b3-f924-45a5-fe7e-a76be8c0bfdc"
      },
      "source": [
        " #here predicting 7737 and 449 samples \n",
        "text_bad = training_data[7737]\n",
        "text_good = training_data[449]\n",
        "texts = (text_bad, text_good)\n",
        "padded_texts = pad_sequences(texts, maxlen=max_sequence_length, padding='post')\n",
        "decoded_predict_7737 = \" \".join( [reverse_index.get(i - 3, \"#\") for i in training_data[7737]] )\n",
        "decoded_predict_449 = \" \".join( [reverse_index.get(i - 3, \"#\") for i in training_data[449]] )\n",
        "\n",
        "print(decoded_predict_7737)\n",
        "print(decoded_predict_449)\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# as a # and non christian i thought i really was going to be holding onto my faith but what a load of i # i thought the film would have great arguments but only got one sided views from # and jews and who are all these street people he's # who don't know the back of their arm from their head where are the proper # and priests and stuff he could have got arguments from not retired nuts who wrote books and finished their studies in 1970 personally this dvd was a waste of time and not worth my time to check if the facts are right or wrong or if i should or should not believe because an anti christ told me so please to think he came up with the conclusion of not finding god because his own ego and demons got the better of him no im not going to say the movie was stunning to help # reading this feel better about themselves but if you really want to show the world you care about us poor souls who believe in jesus then # us with your worth not your beating off the drums\n",
            "# here it is the first ever episode of friends where we get introduced to control freak monica # # cox newly divorced ross # david # # # # lisa # unknown actor and ladies man matt le # and very sarcastic chandler bing matthew perry this is how the scene starts off until we introduced to the # and final friend # kid rachel green jennifer aniston br br the episode is better than most people give credit for like any new sitcom the first episode isn't always fantastic the acting in this episode isn't great because the cast cannot identify and # really believable in their new characters apart from # and perry who shine br br matt le # man his acting was down right dreadful because until later he gets more confident but i think he tries to be funny but at most fails br br david # why does he over # every word he cannot speak normally but he became one of the funniest characters in later seasons but he isn't confident and i cannot # with him jennifer aniston looks hot and does a good job as rachel green but we only see the real rachel later in the 1st season # cox looks quite # in this episode its worrying she looks totally different now more # she acting is a little # but # is in this 20 minute pilot lisa # and matthew perry i'm doing these two together because their comic timing and acting quality was superb and for lisa this was one of her first roles and she is so natural as # # and matthew perry is just matthew perry playing himself basically the episode quality does improve later such as the sets they looks dark and creepy in this episode and makes them seem # the acting is ok the characters gain confidence with each new scene and i am proud this is the pilot i hope we see the friends reunite cause they will always be there for us\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMB3JuTPk5QH",
        "outputId": "cc1f01e0-7458-432b-f6da-274b47b9ad57"
      },
      "source": [
        "predictions =mod3.predict(padded_texts)\n",
        "print(predictions[0].mean())\n",
        "print(predictions[1].mean())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1501305\n",
            "0.9046873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdbXlqq17W6a"
      },
      "source": [
        "##Conclusion:\n",
        "##7737-predictions[0]    close to 0,  which is bad. Obviously, this is correct based on  the text.\n",
        "##449 -predictions[1] is close to 1 which is Good.This makes sense â the text clearly indicates that the viewer had positive sentiment about the movie.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp5YTneFq0pd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}